{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 02\n",
    "Name: Marie Nguyen \n",
    "#### Part 1:\n",
    "\n",
    "Modify your code for lab 1 so that it:\n",
    "\n",
    "a. parses only the headline and text sections of each document.\n",
    "\n",
    "b. can run with stopping and/or stemming either on or off\n",
    "\n",
    "In part 1, I am going to parse document number, headline and text from the XML file collection and write these elements into a file for comparision when stopping or stemming is gradually on or off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "import string\n",
    "import xml.etree.ElementTree as ET\n",
    "# Sources: https://docs.python.org/3/library/xml.etree.elementtree.html \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "# Resource: https://realpython.com/nltk-nlp-python/#stemming\n",
    "\n",
    "# Define a function for viewing all deadlines and texts of the collection when stopping is on\n",
    "def stopping_file(input, output):\n",
    "    \"\"\" Parameters: input: original headlines and texts from the collection\n",
    "    output: document with all headlines and text removed stopwords\"\"\"\n",
    "    \n",
    "    stopwords_array = []\n",
    "    \n",
    "    # Open and read the stopword file\n",
    "    with open(\"stoplist\", \"r\") as stoplistFile:\n",
    "        stopwords_array = stoplistFile.read().lower().split() #Tokenization\n",
    "    \n",
    "    with open(input, \"r\") as input_file, open(output, \"w\") as output_file:\n",
    "        lines = input_file.readlines() \n",
    "        for line in lines:\n",
    "            # Remove punctuation with a space (when its length = puctuation's length)\n",
    "            punctTable = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "            line = line.translate(punctTable)\n",
    "            # Tokenization to create a list of words \n",
    "            words = line.split() \n",
    "            # Interate each words in the list of words\n",
    "            for word in words:\n",
    "            # Check if the word is not in the stop words array\n",
    "                if word not in stopwords_array:\n",
    "                    # Add the word to the output file if it's not a stopword\n",
    "                    output_file.write(word + \" \")\n",
    "            output_file.write(\"\\n\")\n",
    "        \n",
    "    # Close the files\n",
    "    input_file.close()\n",
    "    output_file.close()\n",
    "    \n",
    "# Define a function for viewing all deadlines and texts of the collection when stemming is on\n",
    "def stemming_file(input, output):\n",
    "    with open(input, \"r\") as input_file, open(output, \"w\") as output_file:\n",
    "        lines = input_file.readlines()\n",
    "        for line in lines:\n",
    "            # Remove punctuation with a space (when its length = puctuation's length)\n",
    "            punctTable = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "            line = line.translate(punctTable)\n",
    "            # Tokenization to create a list of words \n",
    "            words = line.split() \n",
    "            # Interate each words in the list of words\n",
    "            for word in words:\n",
    "                word = stemmer.stem(word) #Nomalization\n",
    "                output_file.write(word + \" \")\n",
    "            output_file.write(\"\\n\")\n",
    "\n",
    "    # Close the files\n",
    "    input_file.close()\n",
    "    output_file.close()\n",
    "\n",
    "# Define a parsing function for parsing headline and text of each doc \n",
    "def parsing(input_XML, healine_text_document, stopping, stemming):\n",
    "    \"\"\" Parameter: input XML file, \n",
    "    headline_text_document: a document contains doc number, headline and texts \n",
    "        of the collection for comparision. \n",
    "    stopping; True or False, whether stopping is on or off\n",
    "    stemming: True or False, whether stemming is on or off\n",
    "    Return: A document with all document numbers, headlines, and texts \n",
    "    Use ElementTree module to read the XML file \"\"\"\n",
    "\n",
    "    tree = ET.parse(input_XML) \n",
    "    root = tree.getroot()\n",
    "    \n",
    "    \"\"\" Parsing only the headline and text sections of each document\n",
    "        and save them in a new document \"\"\"\n",
    "\n",
    "    with open(healine_text_document, \"w\") as output_file:\n",
    "        # Loop through each <DOC> element\n",
    "        for doc in root.findall(\"DOC\"):\n",
    "            # Find the headline and text elements\n",
    "            headline = doc.find(\"HEADLINE\")\n",
    "            text = doc.find(\"TEXT\")\n",
    "            # Find document number element\n",
    "            doc_number = doc.find(\"DOCNO\") \n",
    "\n",
    "            # Write document number, headline and text into the output file\n",
    "            if headline is not None and text is not None:\n",
    "                output_file.write(doc_number.text)\n",
    "                output_file.write(\"\\n\")\n",
    "                output_file.write(headline.text)\n",
    "                output_file.write(text.text)\n",
    "                output_file.write(\"\\n\")\n",
    "        \n",
    "    output_file.close()\n",
    "    \n",
    "    if stopping == True:\n",
    "        stopping_file(healine_text_document, \"Parsing_Stopping.txt\")\n",
    "        \n",
    "    if stemming == True:\n",
    "        stemming_file(healine_text_document, \"Parsing_Stemming.txt\")\n",
    "\n",
    "# When stopping is on        \n",
    "parsing(\"trec.sample.xml\", \"Original_parsing.txt\", True, False)\n",
    "# When stemming is on\n",
    "parsing(\"trec.sample.xml\", \"Original_parsing.txt\", False, True)\n",
    "# When stopping and stemming is off\n",
    "parsing(\"trec.sample.xml\", \"Original_parsing.txt\", False, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2:\n",
    "Implement an inverted index for the documents. You need to save the following information for each term:\n",
    "\n",
    "a. term (pre-processed) and it’s document frequency\n",
    "\n",
    "b. list of documents in which this term occurred. (a linked list to store this data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linked list \n",
    "# Source: https://www.geeksforgeeks.org/python-linked-list/ and Data Structure course\n",
    "\n",
    "# Initialize a Node class to create a node \n",
    "class Node:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.next = None\n",
    "\n",
    "# Initialize a Linkedlist class\n",
    "class LinkedList:\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "\n",
    "    def add(self, data):\n",
    "        # Initialize a new node with the data \n",
    "        new_node = Node(data)\n",
    "        # If the Linkedlist is empty, let the new node is the head \n",
    "        if not self.head:\n",
    "            self.head = new_node\n",
    "            return\n",
    "        # Otherwise, let the current node is the head \n",
    "        current = self.head\n",
    "        # Loop through the end of the Linkedlist \n",
    "        while current.next:\n",
    "            current = current.next\n",
    "        # Add a new node to the end. \n",
    "        current.next = new_node\n",
    "                   \n",
    "    def toString(self):\n",
    "        # Assign a current node as the head \n",
    "        current = self.head\n",
    "        # Initialize an empty string for printing out the linkedlist\n",
    "        content = \" \"\n",
    "        # Loop through all nodes in the linked list\n",
    "        while (current != None):\n",
    "            # Add the data of the node to the content \n",
    "            content += \"    \" + str(current.data) + \" \\n \"\n",
    "            # Move to the next node \n",
    "            current = current.next\n",
    "            \n",
    "        # content += \"None\"\n",
    "        return content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stoplist\", \"r\") as stoplistFile:\n",
    "    stopwords_array = stoplistFile.read().lower().split() #Tokenization\n",
    "        \n",
    "# Define a function to return a dict of terms and doc frequency \n",
    "def parsing_terms(input_XML, stopping, stemming):\n",
    "    # Parameter: input XML file, either stopping or stemming is on or off\n",
    "    # Return: Lists of headlines and texts \n",
    "    # Use ElementTree module to read the XML file \n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tree = ET.parse(input_XML) \n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Initialize a dictionary for term and its document frequency \n",
    "    term_doc_dict = {}\n",
    "    \n",
    "    \"\"\" Parsing only the headline and text sections of each document\n",
    "        and save them in a new document \"\"\"\n",
    "        \n",
    "    # Loop through each <DOC> element\n",
    "    for doc in root.findall(\"DOC\"):\n",
    "        terms_array = []\n",
    "        \n",
    "        # Find the headline and text elements\n",
    "        headline = doc.find(\"HEADLINE\")\n",
    "        text_doc = doc.find(\"TEXT\")\n",
    "        # Find document number element \n",
    "        doc_number = doc.find(\"DOCNO\") \n",
    "        \n",
    "        # Remove punctuation and apply lower case\n",
    "        punctTable = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "        headline = headline.text.lower().translate(punctTable)\n",
    "        text_doc = text_doc.text.lower().translate(punctTable)\n",
    "            \n",
    "        # Get unique terms from headline in each document \n",
    "        for term in headline.split():\n",
    "            if term not in terms_array:\n",
    "                # On stemming \n",
    "                if stopping == True: \n",
    "                    if term not in stopwords_array:\n",
    "                        terms_array.append(term)\n",
    "                # On stemming      \n",
    "                elif stemming == True:\n",
    "                    terms_array.append(stemmer.stem(term)) #Nomalization\n",
    "                # Original \n",
    "                else:\n",
    "                    terms_array.append(term)\n",
    "        \n",
    "        # Get unique terms from text in each document \n",
    "        for term in text_doc.split():\n",
    "            if term not in terms_array:\n",
    "                # On stopping \n",
    "                if stopping == True: \n",
    "                    if term not in stopwords_array:\n",
    "                        terms_array.append(term)\n",
    "                # On stemming        \n",
    "                elif stemming == True:\n",
    "                    term = stemmer.stem(term) #Nomalization\n",
    "                    terms_array.append(stemmer.stem(term)) #Nomalization\n",
    "                # Original     \n",
    "                else:\n",
    "                    terms_array.append(term)\n",
    "        \n",
    "        # Loop through the terms array in each document \n",
    "        for word in terms_array:\n",
    "            \"\"\" If the word exists in the document\n",
    "            add the document number to the word's linkedlist \"\"\"\n",
    "            if word in term_doc_dict:\n",
    "                term_doc_dict[word].add(doc_number.text)\n",
    "            else:\n",
    "                term_doc_dict[word] = LinkedList()\n",
    "                term_doc_dict[word].add(doc_number.text)\n",
    "                       \n",
    "    return term_doc_dict     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: \n",
    "\n",
    "Print out for visualization, the output of your inverted index in a text file.\n",
    "Example output can be found in trec.index.txt. Note that this output file was generated without applying stopping or stemming. Also notice that the output is sorted by term. Check your output inverted index when you enable/disable the following:\n",
    "\n",
    "a. stopping\n",
    "\n",
    "b. stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "# Resource: https://www.geeksforgeeks.org/python-sort-python-dictionaries-by-key-or-value/ \n",
    "\n",
    "# Write in a document the output of the inverted index in a text file \n",
    "with open(\"inverted_index_file\", \"w\") as file:\n",
    "    # Call the parsing_terms function in part 2:\n",
    "    term_dictionary = parsing_terms(\"trec.sample.xml\", False, False)\n",
    "    terms_list = OrderedDict(sorted(term_dictionary.items()))\n",
    "\n",
    "    for term in terms_list:\n",
    "        file.writelines(term + \":\" + \"\\n\")\n",
    "        file.writelines(\"\" + term_dictionary[term].toString())  \n",
    "        \n",
    "with open(\"inverted_index_file_stopping\", \"w\") as file:\n",
    "    # Call the parsing_terms function in part 2:\n",
    "    term_dictionary = parsing_terms(\"trec.sample.xml\", True, False)\n",
    "    terms_list = OrderedDict(sorted(term_dictionary.items()))\n",
    "\n",
    "    for term in terms_list:\n",
    "        file.writelines(term + \":\" + \"\\n\")\n",
    "        file.writelines(\"\" + term_dictionary[term].toString())  \n",
    "        \n",
    "with open(\"inverted_index_file_stemming\", \"w\") as file:\n",
    "    # Call the parsing_terms function in part 2:\n",
    "    term_dictionary = parsing_terms(\"trec.sample.xml\", False, True)\n",
    "    terms_list = OrderedDict(sorted(term_dictionary.items()))\n",
    "\n",
    "    for term in terms_list:\n",
    "        file.writelines(term + \":\" + \"\\n\")\n",
    "        file.writelines(\"\" + term_dictionary[term].toString())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Implement a simple word overlap retrieval algorithm using the similarity function. Run the queries in the file queries.lab2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q1', 'scotland']\n",
      "['q2', 'window']\n",
      "['q3', 'replacing']\n",
      "['q4', 'window', 'replacing']\n",
      "['q5', 'condemning']\n",
      "['q6', 'income', 'taxes']\n",
      "['q7', 'middle', 'east', 'peace']\n"
     ]
    }
   ],
   "source": [
    "# This code is for preprocessing step: tokenization of the queries.lab2.txt file \n",
    "with open (\"queries.lab2.txt\", \"r\") as query_file:\n",
    "    queries = query_file.readlines()\n",
    "    punctTable = str.maketrans(\"\",\"\",string.punctuation)\n",
    "    for query in queries:\n",
    "        query = query.strip().translate(punctTable).lower().split()\n",
    "        print(query)    \n",
    "query_file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for a dictionary of terms from the query and their LinkedList docIDs\n",
    "def similarity_search():\n",
    "    query_dictionary = {}\n",
    "    # Call the parsing_terms function in part 2:\n",
    "    term_dictionary = parsing_terms(\"trec.sample.xml\", False, False)\n",
    "    \n",
    "    # Tokenize the query document into words \n",
    "    with open (\"queries.lab2.txt\", \"r\") as query_file,open(\"output\", \"w\") as output_file:\n",
    "        queries = query_file.readlines()\n",
    "        punctTable = str.maketrans(\"\",\"\",string.punctuation)  # Remove punctuation \n",
    "        # Loop through each query row in the file \n",
    "        for query in queries:\n",
    "            query = query.strip().translate(punctTable).lower().split() # Preprocessing \n",
    "            # Loop through terms in the query. [Ignore the first element in the query array cause it's not the term]\n",
    "            for index in range(1,len(query)):\n",
    "                if query[index] in term_dictionary:\n",
    "                    term = query[index]\n",
    "                    # Assign the LinkedList ID of the term in the inverted file to the query dictionary \n",
    "                    query_dictionary[term] = term_dictionary[term] \n",
    "                # If there is a new term, assign an empty LinkedList \n",
    "                else:\n",
    "                    query_dictionary[term] = LinkedList()\n",
    "    \n",
    "    return query_dictionary\n",
    "\n",
    "# Write the do\n",
    "with open(\"test.txt\", \"w\") as file:\n",
    "    # Call the parsing_terms function in part 2:\n",
    "    query_dict = similarity_search()\n",
    "    # Print out the dictionary. \n",
    "    for term in query_dict:\n",
    "        file.writelines(term + \":\" + \"\\n\")\n",
    "        file.writelines(\"\" + query_dict[term].toString())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Implement Boolean search using AND, OR, and NOT. Run the queries in the file queries.boolean.lab2.txt.\n",
    "\n",
    "I am stuck on this part. I will go to office hours to ask for help and finish it later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a functionn for getting the list of document ID from the collection \n",
    "def all_docIDs(input_XML):\n",
    "    tree = ET.parse(input_XML) \n",
    "    root = tree.getroot()\n",
    "    \n",
    "    docIDs_array = LinkedList()\n",
    "    \n",
    "    # Loop through each <DOC> element\n",
    "    for doc in root.findall(\"DOC\"):\n",
    "        # Find document number element \n",
    "        doc_number = doc.find(\"DOCNO\") \n",
    "        docIDs_array.add(doc_number.text)\n",
    "        \n",
    "    return docIDs_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources: https://www.geeksforgeeks.org/evaluation-of-expression-tree/\n",
    "\n",
    "https://www.geeksforgeeks.org/convert-infix-expression-to-postfix-expression/\n",
    "\n",
    "I asked Astrid Zhao for help in this problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "# Define an expression tree for the given query \n",
    "def expression_tree(query):\n",
    "    # Parameter: an array of tokens in the infix query\n",
    "    \"\"\" Convert the infix query to the postfix query and then build an expression tree\n",
    "    so that parent nodes are logical expressions and child nodes are terms in the dictionary\"\"\"\n",
    "    \n",
    "    # Put a score to each logical expression to classify top nodes of building the tree \n",
    "    logical_score = {\"not\": 3, \"or\": 2, \"and\": 1}\n",
    "    # Initialize stacks for postfix expression and logical expression stack \n",
    "    postfix = []\n",
    "    logic_stack = [] \n",
    "    \n",
    "    # Loop through each token the infix expression stack \n",
    "    for token in query:\n",
    "        # If it is a term, add it to the posfix stack \n",
    "        if token not in logical_score: \n",
    "            postfix.append(token)\n",
    "        # If it is a logical expression, add it to the logic stack \n",
    "        elif token == \"not\":\n",
    "            logic_stack.append(token)\n",
    "        else:  # (AND or OR)\n",
    "            # while the logical expression stack is not empty and\n",
    "            # the top element of the stack is not NOT\n",
    "            while (logic_stack and logic_stack[-1] != \"not\"):\n",
    "                # if the top element of the stack has a higher or equal score than \n",
    "                # the current token (preferred \"AND\" in this case)\n",
    "                if logical_score[logic_stack[-1]] >= logical_score[token]:\n",
    "                    # Pop the top logical expression from logic_stack and appends it to the postfix expression \n",
    "                    # To ensure token with higher score are evaluated first \n",
    "                    postfix.append(logic_stack.pop())\n",
    "                    \n",
    "            # Add the current token to the logic stack \n",
    "            logic_stack.append(token)\n",
    "            \n",
    "    # While there is still logical expression in the logic stack \n",
    "    # Remove (.pop) it from the stack and add it to the postfix stack \n",
    "    while logic_stack:\n",
    "        postfix.append(logic_stack.pop())\n",
    "        \n",
    "    # Initialize an empty stack for building a tree \n",
    "    stack = []\n",
    "    logical_expressions = [\"not\", \"or\", \"and\"]\n",
    "\n",
    "    # If token is a term push that into the stack\n",
    "    # If token is an logical expression pop two values from the stack and \n",
    "    # make the 2 values become children node of the logical expression node \n",
    "    # and push the current node into the stack.\n",
    "    for token in postfix:\n",
    "        # If the term is different from logical expression\n",
    "        if token not in logical_expressions:\n",
    "            # Create a term tree node and add to the stack\n",
    "            stack.append(TreeNode(token))\n",
    "        else:\n",
    "            if token == \"not\":\n",
    "                # get the top element from the stack\n",
    "                left_term = stack.pop()\n",
    "                # Create a \"not\" tree node \n",
    "                new_treenode = TreeNode(token)\n",
    "                # Assign the term as the right node of \"not\" node \n",
    "                new_treenode.left = left_term\n",
    "                \n",
    "            else:\n",
    "                # If token is \"and\" or \"or\"\n",
    "                # Get the 2 top element from the stack \n",
    "                right_term = stack.pop()\n",
    "                left_term = stack.pop()\n",
    "                # Create a tree node for \"and\" or \"or\"\n",
    "                new_treenode = TreeNode(token)\n",
    "                # Assign the 2 consecutive terms as the right and left node \n",
    "                new_treenode.right = right_term \n",
    "                new_treenode.left = left_term \n",
    "                \n",
    "                \n",
    "            # Add the logical expression tree node to the stack \n",
    "            stack.append(new_treenode)\n",
    "                \n",
    "    # Return the root node of the expression tree \n",
    "    return stack[0]\n",
    "\n",
    "# Define an or boolean search\n",
    "def Boolean_search_or(term1_docIDs, term2_docIDs):\n",
    "    # Parameters: Linkedlist docIDs of the 2 childs term \n",
    "    # Initialize a result LinkedList\n",
    "    merge_posting = LinkedList()  \n",
    "    \n",
    "    current_docID_term1 = term1_docIDs.head    # Current pointer \n",
    "    current_docID_term2 = term2_docIDs.head    # Current pointer \n",
    "    \n",
    "    # Loop through the 2 LinkedList at the same time \n",
    "    while current_docID_term1 != None and current_docID_term2 != None:\n",
    "        # If docID of the first LinkedList < docID of the second LinkedList \n",
    "        if current_docID_term1.data < current_docID_term2.data:\n",
    "            # Add that docID of the first LinkedList to the result posting \n",
    "            merge_posting.add(current_docID_term1.data)\n",
    "            # Moving the pointer in the first LinkedList \n",
    "            current_docID_term1 = current_docID_term1.next\n",
    "        # If docID of the second LinkedList < docID of the first LinkedList    \n",
    "        elif current_docID_term2.data < current_docID_term1.data:\n",
    "            # Add that docID of the second LinkedList to the result posting \n",
    "            merge_posting.add(current_docID_term2.data)\n",
    "            # Moving the pointer in the second LinkedList \n",
    "            current_docID_term2 = current_docID_term2.next\n",
    "            \n",
    "        else:\n",
    "            # Both LinkedLists have the same docID, add that docID to the result posting \n",
    "            merge_posting.add(current_docID_term2.data)\n",
    "            # Moving the pointers in the 2 LinkedLists\n",
    "            current_docID_term1 = current_docID_term1.next\n",
    "            current_docID_term2 = current_docID_term2.next\n",
    "            \n",
    "    return merge_posting\n",
    "\n",
    "# Define an and boolean search \n",
    "def Boolean_search_and(term1_docIDs, term2_docIDs):\n",
    "    # Parameters: Linkedlist docIDs of the 2 childs term \n",
    "    # Initialize a result LinkedList\n",
    "    merge_posting = LinkedList()\n",
    "    \n",
    "    current_docID_term1 = term1_docIDs.head   # Current pointer \n",
    "    current_docID_term2 = term2_docIDs.head   # Current pointer \n",
    "    \n",
    "    # Loop through the first LinkedList: \n",
    "    while current_docID_term1 != None: \n",
    "        current_docID_term2 = term2_docIDs.head   # Current pointer\n",
    "        # Loop through the second LinkedList \n",
    "        while current_docID_term2 != None:\n",
    "            # Both LinkedLists have the same docID, add that docID to the result posting \n",
    "            if current_docID_term1.data == current_docID_term2.data:\n",
    "                merge_posting.add(current_docID_term1.data)\n",
    "                break\n",
    "            # Moving the pointer in the second LinkedList\n",
    "            current_docID_term2 = current_docID_term2.next\n",
    "        # Moving the pointer in the first LinkedList    \n",
    "        current_docID_term1 = current_docID_term1.next\n",
    "            \n",
    "    return merge_posting\n",
    "\n",
    "def Boolean_search_not(term_docIDs):\n",
    "    # Parameters: Linkedlist docIDs of the \"not\"'s child term \n",
    "    # Call the LinkedList of all docIDs collection \n",
    "    all_docID_list = all_docIDs(\"trec.sample.xml\")\n",
    "    # Initialize an empty LinkedList to store docIDs that not in the term's docIDs \n",
    "    posting = LinkedList()\n",
    "    \n",
    "    current_docID_term = term_docIDs.head   # Current pointer \n",
    "    current_all_docIDs = all_docID_list.head   # Current pointer \n",
    "\n",
    "    # Loop through the all docIDs collection \n",
    "    while current_all_docIDs != None:\n",
    "        # Compare docID in the term LinkedList to docID in the collection \n",
    "        if current_docID_term == None or current_docID_term.data == None or current_all_docIDs.data < current_docID_term.data:\n",
    "            # Add that docID to the result LinkedList \n",
    "            posting.add(current_all_docIDs.data)\n",
    "            # Moving the pointer of the all docIDs collection \n",
    "            current_all_docIDs = current_all_docIDs.next\n",
    "            \n",
    "        # Both LinkedLists have the same docID, skip this docID \n",
    "        elif current_all_docIDs.data == current_docID_term.data:\n",
    "            # Moving the pointers in the 2 LinkedLists \n",
    "            current_docID_term = current_docID_term.next\n",
    "            current_all_docIDs = current_all_docIDs.next\n",
    "            \n",
    "        else:\n",
    "            # Moving the pointer in the term LinkedList \n",
    "            current_docID_term = current_docID_term.next\n",
    "\n",
    "    return posting\n",
    "\n",
    "# Define a function to evaluate the expression tree\n",
    "def Boolean_retrieval_expression_tree(root):\n",
    "    # Parameter: the root of the expression tree \n",
    "    query_dictionary = similarity_search() \n",
    "    \n",
    "    # Return non if it is an empty tree \n",
    "    if root is None:\n",
    "        return LinkedList()\n",
    "\n",
    "    # Apply the operator represented by the current node\n",
    "    if root.data == \"and\":\n",
    "        # Evaluate left and right subtrees\n",
    "        left_result = Boolean_retrieval_expression_tree(root.left)\n",
    "        right_result = Boolean_retrieval_expression_tree(root.right)\n",
    "        # Evaluate the \"AND\"\n",
    "        return Boolean_search_and(left_result, right_result)\n",
    "    \n",
    "    elif root.data == \"or\":\n",
    "        # Evaluate left and right subtrees\n",
    "        left_result = Boolean_retrieval_expression_tree(root.left)\n",
    "        right_result = Boolean_retrieval_expression_tree(root.right)\n",
    "        # Evaluate the \"OR\"\n",
    "        return Boolean_search_or(left_result,right_result)\n",
    "    \n",
    "    elif root.data == \"not\":\n",
    "        # Evaluate the right subtree:\n",
    "        right_result = Boolean_retrieval_expression_tree(root.right)\n",
    "        # Evaluate the \"NOT \"\n",
    "        return Boolean_search_not(right_result) \n",
    "        \n",
    "    else: # Base case\n",
    "        # Leaf node, return the corresponding linked list\n",
    "        term = root.data\n",
    "        return(query_dictionary[term])\n",
    "\n",
    "\n",
    "with open (\"queries.boolean.lab2.txt\", \"r\") as query_file, open(\"testBoolean.txt\", \"w\") as output_file:\n",
    "    queries = query_file.readlines()\n",
    "    punctTable = str.maketrans(\"\",\"\",string.punctuation) # Remove punctuation \n",
    "    # Read each querry line in the queries.lab2.txt doc\n",
    "    for query in queries:\n",
    "        # Tokenize the query into words with lower case and without punctuation \n",
    "        query = query.strip().lower().translate(punctTable).split()\n",
    "        \n",
    "        output_file.writelines(query[0])\n",
    "        output_file.writelines(\"\\n\")\n",
    "        # Initialize a new query array containing terms and logical expression \n",
    "        new_query = []\n",
    "        for index in range(1,len(query)):\n",
    "            new_query.append(query[index])\n",
    "            \n",
    "        # Initialize an expression tree given the new query \n",
    "        root = expression_tree(new_query)\n",
    "        \n",
    "        # Calling the Boolean retrieval function for a list of docIDs \n",
    "        posting_IDs = Boolean_retrieval_expression_tree(root)\n",
    "        \n",
    "        # Write the result list into the output file.\n",
    "        output_file.writelines(posting_IDs.toString()) \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Report the list of documents retrieved for each query.\n",
    "\n",
    "i. How do the word overlap results compare to the Boolean results?\n",
    "\n",
    "When there is only 1 term in the query, the result of the word overlap and the Boolean is the same. Otherwise, the word overlap results all document IDs that contain the terms in the query while Boolean search return only document IDs that exactly match the query. Taken \"q4: window AND replacin\"g as an example, the word overlap results in document IDs having the 2 terms (window and replacing) while the Boolean search results only 1 document. Hence, the Boolean retrieval system is stricter than the word overlap retrieval system. \n",
    "\n",
    "ii. Where it makes sense, compare and contrast the results for different versions of the same query between the Boolean retrieval and word overlap retrieval (e.g. “middle AND east AND peace” versus “middle east peace”)\n",
    "\n",
    "The result from the Boolean query \"middle AND east AND peace\" will sort out and retrieve document IDs that contain all 3 terms while the word overlap query \"middle east peace\" will consider the similarity score of documents to the query retrieve documents that closely match the query's terms. Hence, the Boolean retrieval results will be more precise while the results of the word overlap retrieval is flexibile including partial match documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) These collections are quite small. What do you expect would happen if you run your system as implemented on a very large collection?\n",
    "\n",
    "If we have a very large collection, both computing similarity function in word overlap retrieval and returning the posting list of document ID in Boolean retrieval systems will take longer time and can be computationally expensive. In addition, my computer might not have enough storage to store the Linkedlist document IDs of terms and preprocessing such as stemming and stopping might take more time to proceed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
